








import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
path="./gym_members_exercise_tracking.csv"
gym=pd.read_csv(path,sep=",",header=0)
gym.head()


gym["Gender"]=pd.Categorical(gym["Gender"],ordered=False)
gym["Workout_Type"]=pd.Categorical(gym["Workout_Type"],ordered=False)
gym["Experience_Level"]=pd.Categorical(gym["Experience_Level"],ordered=True)
gym.dtypes


gym["LWeight"]=gym["Weight (kg)"].map(lambda x:np.log(x))
gym["LBMI"]=gym["BMI"].map(lambda x:np.log(x))
#gym["LWater_Intake"]=gym["Water_Intake (liters)"].map(lambda x:np.log(x))
#gym["Fat_Percentage_S"]=gym["Fat_Percentage"].map(lambda x:x**2/100)
#gym["SHeight"]=gym["Height (m)"].map(lambda x:np.sqrt(x))
del gym["Weight (kg)"]
del gym["BMI"]
#del gym["Water_Intake (liters)"]
#del gym["Fat_Percentage"]
#del gym["Height (m)"]
gym.head()


print(gym.isnull().sum())








from sklearn.preprocessing import StandardScaler  
gym_dummies=pd.get_dummies(gym[["Gender","Workout_Type","Experience_Level"]])
gym_quant=gym[["Age","Height (m)","Max_BPM","Avg_BPM","Resting_BPM","Session_Duration (hours)","Fat_Percentage","Water_Intake (liters)","Workout_Frequency (days/week)","LWeight","LBMI"]]
X=pd.concat([gym_dummies,gym_quant],axis=1)
Y=gym["Calories_Burned"]
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42) # Taille de l'échantillon test : 20%


# L'algorithme des réseaux de neurones ainsi que SVR nécessitent éventuellement une standardisation 
# des variables explicatives avec les commandes ci-dessous
scaler = StandardScaler()  
scaler.fit(X_train)  
Xr_train = scaler.transform(X_train)  
# Meme transformation sur le test
Xr_test = scaler.transform(X_test)








from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score
from sklearn.feature_selection import SequentialFeatureSelector


reglin=LinearRegression().fit(X_train,Y_train)
reglinpred=reglin.predict(X_test)
reglin_coeff = pd.DataFrame(columns=X.columns)
reglin_coeff.loc[1]=reglin.coef_
reglin_coeff


MSE_RLt = mean_squared_error(reglinpred,Y_test)
R2_RLt = r2_score(Y_test,reglinpred)
n = X_test.shape[0]
p = X_test.shape[1]
adjusted_R2_RLt = 1 - (1 - R2_RLt) * (n - 1) / (n - p - 1)
PrevError_RLt = 1 - reglin.score(X_train, Y_train)
print("MSE=",MSE_RLt)
print("R2=",R2_RLt)
print("Adjusted R2=",adjusted_R2_RLt)
print("Erreur sur l'échantillon d'apprentissage = ", PrevError_RLt)





plt.plot(reglinpred,Y_test-reglinpred,"o")
plt.xlabel(u"Valeurs Prédites")
plt.ylabel(u"Résidus")
plt.hlines(0,min(reglinpred),max(reglinpred),"r")
plt.show()





#pip install statsmodels


import statsmodels.api as sm
import pylab

residuals = Y_test - reglinpred
residuals = residuals / residuals.std()
sm.qqplot(residuals, line='45')
pylab.show()








def compute_aic(n, mse, k):
    return n * math.log(mse) + 2 * k

sfs_lin = SequentialFeatureSelector(LinearRegression(), direction="forward", cv=5)
sfs_lin.fit(X_train, Y_train)
X_forward_train=sfs_lin.transform(X_train)
X_forward_test=sfs_lin.transform(X_test)
selected_features = X.columns[sfs_lin.get_support()]
print("Variables sélectionnées :", selected_features)
reglin_forward=LinearRegression().fit(X_forward_train,Y_train)
reglin_forward_pred=reglin_forward.predict(X_forward_test)


reglin_forward_coeff = pd.DataFrame(columns=selected_features)
reglin_forward_coeff.loc[1]=reglin_forward.coef_
reglin_forward_coeff


MSE_RLfs = mean_squared_error(reglin_forward_pred,Y_test)
R2_RLfs = r2_score(Y_test, reglin_forward_pred,)
PrevError_RLfs = 1 - reglin_forward.score(X_forward_train, Y_train)
print("MSE=",MSE_RLfs)
print("R2=", R2_RLfs)
print("Erreur sur l'échantillon d'apprentissage =",PrevError_RLfs)





plt.plot(reglinpred,Y_test-reglin_forward_pred,"o")
plt.xlabel(u"Valeurs Prédites")
plt.ylabel(u"Résidus")
plt.hlines(0,min(reglin_forward_pred),max(reglin_forward_pred),"r")
plt.show()


residuals = Y_test - reglin_forward_pred
residuals = residuals / residuals.std()
sm.qqplot(residuals, line='45')
pylab.show()








from sklearn.linear_model import Lasso
from sklearn.model_selection import GridSearchCV
model=Lasso()
param=[{"alpha":[0.05]+[0.1*i for i in range(1,9)]+[1,2]}]
regLasso = GridSearchCV(Lasso(), param,cv=5,n_jobs=-1)
regLassOpt=regLasso.fit(X_train, Y_train)
regLassOpt.best_params_["alpha"]
print("Meilleur R2 = %f, Meilleur paramètre = %s" % (regLassOpt.best_score_,regLassOpt.best_params_))





prev=regLassOpt.predict(X_test)
MSE_RLasso = mean_squared_error(prev,Y_test)
R2_RLasso = r2_score(Y_test,prev)
PrevError_RLasso = 1 - regLassOpt.score(X_train, Y_train)
print("MSE=",MSE_RLasso)
print("R2=",R2_RLasso)
print(f"Erreur sur l'échantillon d'apprentissage : {PrevError_RLasso}")


p=np.min(Y_test),np.max(Y_test)
plt.plot(Y_test,prev,"o")
plt.plot(p,p,"r-")
plt.xlabel("Calories Brûlées Observées")
plt.ylabel(u"Calories Brûlées Prédites")
plt.show()


plt.plot(prev,Y_test-prev,"o")
plt.xlabel(u"Prédites")
plt.ylabel(u"Résidus")
plt.hlines(0,min(prev),max(prev),"r")
plt.show()








from sklearn.svm import SVR
param={"C":np.arange(1,2000,500), "epsilon":np.arange(0.01,0.5,0.05)}
svr = GridSearchCV(SVR(kernel = 'linear'),param,cv=5,n_jobs=-1)
svrOpt=svr.fit(Xr_train, Y_train)
# paramètre optimal
print("Meilleur score = %f, Meilleur paramètre = %s" % (1. - svrOpt.best_score_,svrOpt.best_params_))
print(svrOpt.best_params_)





# erreur de prévision sur le test
pred_SVR = svrOpt.predict(Xr_test)
MSE_SVR = mean_squared_error(pred_SVR,Y_test)
R2_SVR = r2_score(Y_test,pred_SVR)
PrevError_SVR = 1 - svrOpt.score(Xr_train, Y_train)
print("MSE=",MSE_SVR)
print("R2=",R2_SVR)
print(f"Erreur sur l'échantillon d'apprentissage : {PrevError_SVR}")








from sklearn.tree import DecisionTreeRegressor
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt





# Optimisation d'hyperparamètres
param=[{"max_depth":list(range(2,12)),"min_samples_split":list(range(2,10)),"min_samples_leaf":list(range(2,10))}]

tree= GridSearchCV(DecisionTreeRegressor(),param,cv=5,n_jobs=-1)
treeOpt=tree.fit(X_train, Y_train)
# paramètre optimal
print("Meilleur score = %f, Meilleur paramètre = %s" % (1. - treeOpt.best_score_,treeOpt.best_params_))


# prévision de l'échantillon test avec l'arbre optimal
bestTree=treeOpt.best_estimator_
Y_hat = bestTree.predict(X_test)

plt.figure(figsize=(20,10))
plot_tree(bestTree,feature_names= X.columns.tolist());
plt.show()


#visualisation des predictions

p=np.min(Y_test),np.max(Y_test)
plt.plot(Y_test,Y_hat,"o")
plt.plot(p,p,"r-")
plt.xlabel("Calories Brûlées Observées")
plt.ylabel(u"Calories Brûlées Prédites")
plt.show()





# Estimation des erreurs
MSE_TREE = mean_squared_error(Y_hat,Y_test)
R2_TREE = r2_score(Y_test,Y_hat)
PrevError_TREE = 1-treeOpt.score(X_train,Y_train)
print(f"Erreur sur l'échantillon d'apprentissage : {PrevError_TREE}")
print("MSE=",MSE_TREE)
print("R2=",R2_TREE)





plt.plot(Y_hat,Y_test-Y_hat,"o")
plt.xlabel(u"Prédites")
plt.ylabel(u"Résidus")
plt.hlines(0,min(Y_hat),max(Y_hat),"r")
plt.show()











from sklearn.ensemble import RandomForestRegressor


#Recherche des hyperparamètres optimaux (attention, prend du temps à run)

param=[{"max_features":list(range(5,15,1))+["sqrt"]+["log2"],"max_depth":list(range(5,13,1)),"min_samples_split":list(range(2,7,1)),"min_samples_leaf":list(range(2,7,1))}]
rf= GridSearchCV(RandomForestRegressor(n_estimators=100,bootstrap=True, oob_score=True),
        param,cv=10,n_jobs=-1)
rfOpt=rf.fit(X_train, Y_train)
# paramètre optimal
print("Meilleur score = %f, Meilleurs paramètres = %s" % (1. - rfOpt.best_score_,rfOpt.best_params_))






# définition des paramètres (on reprend les paramètres optimaux trouvés, si la cellule précédente met trop de temps à tourner vous pouvez la sauter)
forest = RandomForestRegressor(n_estimators=500, 
   max_depth=10,
   min_samples_split=3, min_samples_leaf=2, 
   max_features=14, max_leaf_nodes=None,
   bootstrap=True, oob_score=True)
# apprentissage
rfFit = forest.fit(X_train,Y_train)
print(f"Erreur Out-of-Bag: {1-rfFit.oob_score_}")


# erreur de prévision sur le test*
Y_hat = forest.predict(X_test)
MSE_RTREE = mean_squared_error(Y_hat,Y_test)
R2_RTREE = r2_score(Y_test,Y_hat)
PrevError_RTREE = 1-rfFit.score(X_train,Y_train)
print(f"Erreur sur l'échantillon d'apprentissage: {PrevError_RTREE}")
print("MSE=",MSE_RTREE)
print("R2=",R2_RTREE)





# prévision et visualisation
Y_hat = rfFit.predict(X_test)

#visualisation des predictions en fonction des observations
p=np.min(Y_test),np.max(Y_test)
plt.plot(Y_hat,Y_test,"o")
plt.plot(p,p,"r-")
plt.xlabel("Calories Brûlées Observées")
plt.ylabel("Calories Brûlées Prédites")
plt.show()


# Importance décroissante des variables
importances = rfFit.feature_importances_
indices = np.argsort(importances)[::-1]
for f in range(X_train.shape[1]):
    print(X.columns[indices[f]], importances[indices[f]])

plt.figure()
plt.title("Importance des variables")
plt.bar(range(X_train.shape[1]), importances[indices]);
plt.xticks(range(X_train.shape[1]), indices);
plt.xlim([-1, X_train.shape[1]]);
plt.show()








from sklearn.ensemble import GradientBoostingRegressor


# Gradient Boosting Regressor
model = GradientBoostingRegressor(random_state=42)
param_grid = {
    'n_estimators': [300, 500, 1000],
    'learning_rate': [0.02, 0.05, 0.1],
    'max_depth': [2, 3, 5],
    'subsample': [0.8, 0.5, 0.3],
}

# Grid Search with 5-fold cross-validation
grid = GridSearchCV(model, param_grid, cv=5, n_jobs=-1)
GBRopt = grid.fit(X_train, Y_train)
print("Meilleur score = %f, Meilleurs paramètres = %s" % (1. - GBRopt.best_score_,GBRopt.best_params_))


gbr = GradientBoostingRegressor(
    n_estimators=1000,    # number of boosting stages
    learning_rate=0.05,   # shrinkage step size
    max_depth=2,         # depth of individual trees
    subsample = 0.5,
    random_state=42
)
gbr.fit(X_train, Y_train)

# Predictions
Y_hat = gbr.predict(X_test)
MSE_GBR = mean_squared_error(Y_hat,Y_test)
R2_GBR = r2_score(Y_test,Y_hat)
PrevError_GBR = 1-rfFit.score(X_train,Y_train)
print(f"Erreur sur l'échantillon d'apprentissage : {PrevError_RTREE}")
print("MSE=",MSE_RTREE)
print("R2=",R2_RTREE)








from sklearn.neural_network import MLPRegressor





param_grid=[{"hidden_layer_sizes":list([(5,),(10,),(15,),(20,)])}]
nnet= GridSearchCV(MLPRegressor(max_iter=20000),param_grid,cv=5,n_jobs=-1) 
# entrainement
nnetOpt=nnet.fit(Xr_train, Y_train)
# paramètre optimal
print("Meilleur score = %f, Meilleur paramètre = %s" % (1. - nnetOpt.best_score_,nnetOpt.best_params_))


# prévision de l'échantillon test
Y_hat = nnetOpt.predict(Xr_test)

#visualisation des predictions en fonction des observations
p=np.min(Y_test),np.max(Y_test)
plt.plot(Y_hat,Y_test,"o")
plt.plot(p,p,"r-")
plt.xlabel("Calories Brûlées Observées")
plt.ylabel("Calories Brûlées Prédites")
plt.show()


# Estimation de l'erreur de prévision sur le test
MSE_NN = mean_squared_error(Y_hat,Y_test)
R2_NN = r2_score(Y_test,Y_hat)
PrevError_NN = 1-nnetOpt.score(Xr_train,Y_train)
print("Erreur de prévision = %f" % (PrevError_NN))
print("MSE=",MSE_NN)
print("R2=",R2_NN)











recap = pd.DataFrame({"Erreur sur l'échantillon d'apprentissage": [PrevError_RLt, PrevError_RLfs, PrevError_RLasso, PrevError_SVR, PrevError_TREE, PrevError_RTREE, PrevError_NN], 'ME': np.sqrt([MSE_RLt, MSE_RLfs, MSE_RLasso, MSE_SVR, MSE_TREE, MSE_RTREE, MSE_NN]), 'R2': [R2_RLt, R2_RLfs, R2_RLasso, R2_SVR, R2_TREE, R2_RTREE, R2_NN]}, index=['RegLinTot','RegLinFSelec', 'RegLasso', 'SVR','Tree','RandomTree', 'NeuralNetwork'])


recap











regLasso=Lasso(alpha=regLassOpt.best_params_['alpha'])
model_lasso=regLasso.fit(X_train,Y_train)
coef = pd.Series(model_lasso.coef_, index = X_train.columns)
print("Lasso conserve " + str(sum(coef != 0)) + " variables et en supprime " +  str(sum(coef == 0)))
imp_coef = coef.sort_values()
plt.rcParams['figure.figsize'] = (5.0, 6.0)
imp_coef.plot(kind = "barh")
plt.title(u"Coefficients du modèle LASSO")








gym_dummies=pd.get_dummies(gym[["Gender","Workout_Type"]])
gym_quant=gym[["Age","Height (m)","Max_BPM","Avg_BPM","Resting_BPM","Session_Duration (hours)","Fat_Percentage","Water_Intake (liters)","Workout_Frequency (days/week)","LWeight","LBMI", "Calories_Burned"]]
X=pd.concat([gym_dummies,gym_quant],axis=1)
Y=gym["Experience_Level"]
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42) # Taille de l'échantillon test : 20%


# L'algorithme des réseaux de neurones ainsi que SVR nécessitent éventuellement une standardisation 
# des variables explicatives avec les commandes ci-dessous
scaler = StandardScaler()  
scaler.fit(X_train)  
Xr_train = scaler.transform(X_train)  
# Meme transformation sur le test
Xr_test = scaler.transform(X_test)





from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score


logit = LogisticRegression(penalty="l1",solver="liblinear")
logitOpt=logit.fit(X_train, Y_train)  # GridSearchCV est lui-même un estimateur
Y_hat = logitOpt.predict(X_test)
table = pd.crosstab(Y_hat, Y_test)
print(table)
print(accuracy_score(Y_test, Y_hat))











# Optimisation du paramètre de pénalisation
# grille de valeurs
param=[{"C":[1,2,3,5,10]}]
logitP = GridSearchCV(LogisticRegression(penalty="l1",solver="liblinear"), param,cv=5,n_jobs=-1)
logitOptP=logitP.fit(X_train, Y_train)  # GridSearchCV est lui-même un estimateur
# paramètre optimal
logitOptP.best_params_["C"]
print("Meilleur score = %f, Meilleur paramètre = %s" % (logitOptP.best_score_,logitOptP.best_params_))


Y_hat = logitOptP.predict(X_test)
table = pd.crosstab(Y_hat, Y_test)
print(table)
print(accuracy_score(Y_test, Y_hat))











from sklearn.decomposition import PCA
from sklearn.cluster import KMeans


pca=PCA(6)
scaler.fit(gym_quant)  
gym_acp = scaler.transform(gym_quant)  
X_acp=pca.fit_transform(gym_acp)


kmeans=KMeans(n_clusters=3)
k_clusters=kmeans.fit_predict(X_acp)
table = pd.crosstab(k_clusters, gym["Experience_Level"])


print(table)
print(accuracy_score(gym["Experience_Level"], k_clusters))








from sklearn.neighbors import KNeighborsClassifier
# Optimisation de k
# grille de valeurs
param_grid=[{"n_neighbors":list(range(1,15))}]
knn=GridSearchCV(KNeighborsClassifier(),param_grid,cv=5,n_jobs=-1)
knnOpt=knn.fit(Xr_train, Y_train)  # GridSearchCV est lui même un estimateur
# paramètre optimal
knnOpt.best_params_["n_neighbors"]
print("Meilleur score = %f, Meilleur paramètre = %s" % (1.-knnOpt.best_score_,knnOpt.best_params_))





Y_hat = knnOpt.predict(Xr_test)
table = pd.crosstab(Y_hat, Y_test)
print(table)
print(accuracy_score(Y_test, Y_hat))











from sklearn.svm import SVC
param=[{"C":[0.1, 0.15, 0.2, 0.25, 0.3, 0.5], "gamma" : ['scale', 'auto']}]
svm= GridSearchCV(SVC(),param,cv=5,n_jobs=-1)
svmOpt=svm.fit(Xr_train, Y_train)
# paramètre optimal
print("Meilleur score = %f, Meilleur paramètre = %s" % (1. - svmOpt.best_score_,svmOpt.best_params_))


Y_hat = svmOpt.predict(Xr_test)
table = pd.crosstab(Y_hat, Y_test)
print(table)
print(accuracy_score(Y_test, Y_hat))








from sklearn.tree import DecisionTreeClassifier
# Optimisation de la profondeur de l'arbre
param=[{"max_depth":list(range(2,10))}]
tree= GridSearchCV(DecisionTreeClassifier(),param,cv=5,n_jobs=-1)
treeOpt=tree.fit(X_train, Y_train)
# paramètre optimal
print("Meilleur score = %f, Meilleur paramètre = %s" % (1. - treeOpt.best_score_,treeOpt.best_params_))


Y_hat = treeOpt.predict(X_test)
table = pd.crosstab(Y_hat, Y_test)
print(table)
print(accuracy_score(Y_test, Y_hat))





from sklearn.tree import plot_tree
import matplotlib.pyplot as plt
treeG=DecisionTreeClassifier(max_depth=treeOpt.best_params_['max_depth'])
treeG.fit(X_train,Y_train)
plot_tree(treeG,feature_names=gym.columns.tolist());
plt.show()



